{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e42a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 데이터 로드 중...\n",
      "🔍 결측 제거 및 열 추출 중...\n",
      "🧠 SBERT 임베딩 모델 로딩 중...\n",
      "🔡 텍스트 임베딩 중 (SBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💬 임베딩 진행:   8%|█▋                   | 10/123 [02:47<31:30, 16.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# ✅ 문장 임베딩\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m X_embed \u001b[38;5;241m=\u001b[39m \u001b[43membed_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# ✅ One-hot 인코딩 처리\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔠 분류 및 감성 One-hot 인코딩 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36membed_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     32\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m💬 임베딩 진행\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:350\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    347\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 350\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# ✅ 시작\n",
    "print(\"🚀 데이터 로드 중...\")\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\파프\\기사_전체_통합본.xlsx\")\n",
    "df[\"기사수\"] = pd.to_numeric(df[\"기사수\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "X_news_count = df[[\"기사수\"]].values  # shape (n, 1)\n",
    "\n",
    "print(\"🔍 결측 제거 및 열 추출 중...\")\n",
    "category_cols = ['통합 분류1', '통합 분류2', '통합 분류3',\n",
    "                 '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3', '감성']\n",
    "df = df.dropna(subset=[\"설명력\", \"특성추출(가중치순 상위 50개)\"])\n",
    "y = df[\"설명력\"].astype(int)\n",
    "\n",
    "# ✅ SBERT 모델 로드\n",
    "print(\"🧠 SBERT 임베딩 모델 로딩 중...\")\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "# ✅ 임베딩 함수\n",
    "def embed_features(df):\n",
    "    print(\"🔡 텍스트 임베딩 중 (SBERT)...\")\n",
    "    texts = df[\"특성추출(가중치순 상위 50개)\"].fillna(\"\").tolist()\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"💬 임베딩 진행\"):\n",
    "        emb = model.encode(text, convert_to_numpy=True)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ✅ 문장 임베딩\n",
    "X_embed = embed_features(df)\n",
    "\n",
    "# ✅ One-hot 인코딩 처리\n",
    "print(\"🔠 분류 및 감성 One-hot 인코딩 중...\")\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_cat = onehot.fit_transform(df[category_cols])\n",
    "print(f\"  ➤ 인코딩 결과 shape: {X_cat.shape}\")\n",
    "\n",
    "# ✅ 전체 feature 합치기\n",
    "print(\"🔗 임베딩 + 인코딩 합치기...\")\n",
    "X_all = np.concatenate([X_embed, X_cat, X_news_count], axis=1)\n",
    "\n",
    "print(f\"  ➤ 최종 feature shape: {X_all.shape}\")\n",
    "\n",
    "# ✅ 학습-검증 분할\n",
    "print(\"🧪 학습/검증 세트 분할 중...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42)\n",
    "print(f\"  ➤ 학습 세트: {X_train.shape}, 검증 세트: {X_test.shape}\")\n",
    "\n",
    "# ✅ 모델 학습\n",
    "print(\"🎯 랜덤포레스트 분류기 학습 중...\")\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ 학습 완료!\")\n",
    "\n",
    "# ✅ 모델 평가\n",
    "print(\"📊 모델 성능 평가:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec2c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔡 텍스트 임베딩 중 (SBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💬 임베딩 진행: 100%|████████████████████| 123/123 [35:27<00:00, 17.29s/it]\n"
     ]
    }
   ],
   "source": [
    "def embed_features(df):\n",
    "    print(\"🔡 텍스트 임베딩 중 (SBERT)...\")\n",
    "    texts = df[\"특성추출(가중치순 상위 50개)\"].fillna(\"\").tolist()\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"💬 임베딩 진행\"):\n",
    "        emb = model.encode(text, convert_to_numpy=True)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# ✅ 문장 임베딩\n",
    "X_embed = embed_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b29a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pickle\n",
    "\n",
    "with open(r\"C:\\Users\\user\\Desktop\\파프\\X_embed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_embed, f)  # 가장 보존력 좋음\n",
    "pd.DataFrame(X_embed).to_csv(r\"C:\\Users\\user\\Desktop\\파프\\X_embed.csv\", index=False)# 호환성 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5ad395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 데이터 로드 중...\n",
      "🔍 결측 제거 및 열 추출 중...\n",
      "🔠 분류 및 감성 One-hot 인코딩 중...\n",
      "  ➤ 인코딩 결과 shape: (123, 146)\n",
      "🔗 임베딩 + 인코딩 합치기...\n",
      "  ➤ 최종 feature shape: (123, 915)\n",
      "🧪 학습/검증 세트 분할 중...\n",
      "  ➤ 학습 세트: (98, 915), 검증 세트: (25, 915)\n",
      "🎯 랜덤포레스트 분류기 학습 중...\n",
      "✅ 학습 완료!\n",
      "📊 모델 성능 평가:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.92      0.76        12\n",
      "           1       0.88      0.54      0.67        13\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.76      0.73      0.71        25\n",
      "weighted avg       0.77      0.72      0.71        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 데이터 로드 중...\")\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\파프\\기사_전체_통합본.xlsx\")\n",
    "df[\"기사수\"] = pd.to_numeric(df[\"기사수\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "X_news_count = df[[\"기사수\"]].values  # shape (n, 1)\n",
    "\n",
    "print(\"🔍 결측 제거 및 열 추출 중...\")\n",
    "category_cols = ['통합 분류1', '통합 분류2', '통합 분류3',\n",
    "                 '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3', '감성']\n",
    "df = df.dropna(subset=[\"설명력\", \"특성추출(가중치순 상위 50개)\"])\n",
    "y = df[\"설명력\"].astype(int)\n",
    "# ✅ One-hot 인코딩 처리\n",
    "print(\"🔠 분류 및 감성 One-hot 인코딩 중...\")\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_cat = onehot.fit_transform(df[category_cols])\n",
    "print(f\"  ➤ 인코딩 결과 shape: {X_cat.shape}\")\n",
    "\n",
    "# ✅ 전체 feature 합치기\n",
    "print(\"🔗 임베딩 + 인코딩 합치기...\")\n",
    "X_all = np.concatenate([X_embed, X_cat, X_news_count], axis=1)\n",
    "\n",
    "print(f\"  ➤ 최종 feature shape: {X_all.shape}\")\n",
    "\n",
    "# ✅ 학습-검증 분할\n",
    "print(\"🧪 학습/검증 세트 분할 중...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42)\n",
    "print(f\"  ➤ 학습 세트: {X_train.shape}, 검증 세트: {X_test.shape}\")\n",
    "\n",
    "# ✅ 모델 학습\n",
    "print(\"🎯 랜덤포레스트 분류기 학습 중...\")\n",
    "clf = RandomForestClassifier(n_estimators=600, random_state=42, max_depth = 10, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ 학습 완료!\")\n",
    "\n",
    "# ✅ 모델 평가\n",
    "print(\"📊 모델 성능 평가:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1eea0d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:120\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"설명력\": desc,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance, mode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    ")\n",
    "# ---------------------------------------- #\n",
    "# 📌 증강 함수\n",
    "def targeted_augment(X_embed, X_cat, X_news, y, ref_news_values, n_aug=3, noise_level=0.01):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for i in range(len(X_embed)):\n",
    "        for _ in range(n_aug):\n",
    "            noise = np.random.normal(0, noise_level, size=X_embed.shape[1])\n",
    "            embed_aug = X_embed[i] + noise\n",
    "            \n",
    "            # 🎯 기사수 증강: 테스트 분포에서 직접 샘플링\n",
    "            news_aug = np.random.choice(ref_news_values)\n",
    "            \n",
    "            x_combined = np.concatenate([embed_aug, X_cat[i], [news_aug]])\n",
    "            Xs.append(x_combined)\n",
    "            ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "                                  \n",
    "def interpolate_augment(X_embed, X_cat, X_news, y, n_aug=3, news_reference=None):\n",
    "    \"\"\"\n",
    "    - 임베딩, 카테고리, 타겟은 선형 보간\n",
    "    - 기사수(news)는 전체 분포에서 무작위 샘플링하여 분포 왜곡 방지\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    n = len(X_embed)\n",
    "\n",
    "    if news_reference is None:\n",
    "        news_reference = X_news.flatten()\n",
    "\n",
    "    for _ in range(n_aug * n):\n",
    "        # 랜덤하게 두 샘플 선택\n",
    "        i, j = np.random.randint(0, n, size=2)\n",
    "\n",
    "        alpha = np.random.rand()  # 0~1 사이 가중치\n",
    "        embed_mix = (1 - alpha) * X_embed[i] + alpha * X_embed[j]\n",
    "        cat_mix = (1 - alpha) * X_cat[i] + alpha * X_cat[j]\n",
    "        news_mix = np.random.choice(news_reference)  # 전체 분포에서 기사수 추출\n",
    "        y_mix = (1 - alpha) * y[i] + alpha * y[j]\n",
    "\n",
    "        x_combined = np.concatenate([embed_mix, cat_mix, [news_mix]])\n",
    "        Xs.append(x_combined)\n",
    "        ys.append(y_mix)\n",
    "\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------- #\n",
    "# 📊 분포 비교 시각화\n",
    "def compare_three_distributions(X_train_real, X_train_augmented, X_test, y_train_real, y_train_augmented, y_test):\n",
    "    print(\"\\n📊 분포 비교 (원본 vs 증강 vs 테스트)\")\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "\n",
    "    # 1. 기사수 분포\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(X_train_real[:, -1], label='Train (원본)', fill=True)\n",
    "    sns.kdeplot(X_train_augmented[:, -1], label='Train (증강)', fill=True)\n",
    "    sns.kdeplot(X_test[:, -1], label='Test', fill=True)\n",
    "    plt.title(\"📰 기사수 분포\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Y (상관계수) 분포\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(y_train_real, label='Train Y (원본)', fill=True)\n",
    "    sns.kdeplot(y_train_augmented, label='Train Y (증강)', fill=True)\n",
    "    sns.kdeplot(y_test, label='Test Y', fill=True)\n",
    "    plt.title(\"📊 상관계수 분포\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 정량적 Wasserstein 거리도 출력\n",
    "    print(f\"📏 기사수 거리 (원본 vs 테스트):     {wasserstein_distance(X_train_real[:, -1], X_test[:, -1]):.4f}\")\n",
    "    print(f\"📏 기사수 거리 (증강 포함 vs 테스트): {wasserstein_distance(X_train_augmented[:, -1], X_test[:, -1]):.4f}\")\n",
    "    print(f\"📏 Y 거리 (원본 vs 테스트):         {wasserstein_distance(y_train_real, y_test):.4f}\")\n",
    "    print(f\"📏 Y 거리 (증강 포함 vs 테스트):   {wasserstein_distance(y_train_augmented, y_test):.4f}\")\n",
    "\n",
    "# ---------------------------------------- #\n",
    "# 🚀 데이터 로드\n",
    "print(\"🚀 데이터 로드 중...\")\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\파프\\기사_전체_통합본 (상관계수).xlsx\")\n",
    "df[\"기사수\"] = pd.to_numeric(df[\"기사수\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[df[\"설명력\"].isin([0, 1])].copy()\n",
    "df = df.dropna(subset=[\"특성추출(가중치순 상위 50개)\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 📦 임베딩 로드\n",
    "print(\"📦 임베딩 로드 중...\")\n",
    "with open(r\"C:\\Users\\user\\Desktop\\파프\\X_embed.pkl\", \"rb\") as f:\n",
    "    all_X_embed = pickle.load(f)\n",
    "X_embed_all = all_X_embed[df.index]\n",
    "assert X_embed_all.shape[0] == len(df)\n",
    "\n",
    "# 📂 그룹 대표값 구성\n",
    "category_cols = ['통합 분류1', '통합 분류2', '통합 분류3',\n",
    "                 '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3', '감성']\n",
    "grouped_rows = []\n",
    "print(\"🔄 그룹별 대표값 생성 중...\")\n",
    "\n",
    "for (desc, corr), group in df.groupby(['설명력', '상관계수']):\n",
    "    idxs = group.index.tolist()\n",
    "    news_mean = group[\"기사수\"].mean()\n",
    "    mode_row = group[category_cols].mode().iloc[0]\n",
    "    emb_mean = X_embed_all[idxs].mean(axis=0)\n",
    "\n",
    "    row = {\n",
    "        \"설명력\": desc,\n",
    "        \"상관계수\": corr,\n",
    "        \"기사수\": news_mean\n",
    "    }\n",
    "    for col in category_cols:\n",
    "        row[col] = mode_row[col]\n",
    "    grouped_rows.append((row, emb_mean))\n",
    "\n",
    "df_new = pd.DataFrame([r for r, _ in grouped_rows])\n",
    "X_embed = np.array([e for _, e in grouped_rows])\n",
    "\n",
    "# 🔢 범주형 인코딩\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_cat = onehot.fit_transform(df_new[category_cols])\n",
    "X_news = df_new[[\"기사수\"]].values\n",
    "y_all = df_new[\"상관계수\"].astype(float).values\n",
    "\n",
    "# 🔀 데이터 분할\n",
    "print(\"✂️ 데이터 분리 중...\")\n",
    "idx_all = np.arange(len(y_all))\n",
    "train_idx, test_idx = train_test_split(idx_all, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split by index\n",
    "X_embed_train, X_embed_test = X_embed[train_idx], X_embed[test_idx]\n",
    "X_cat_train, X_cat_test = X_cat[train_idx], X_cat[test_idx]\n",
    "X_news_train, X_news_test = X_news[train_idx], X_news[test_idx]\n",
    "y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "# 합치기\n",
    "X_train_real = np.concatenate([X_embed_train, X_cat_train, X_news_train], axis=1)\n",
    "X_test = np.concatenate([X_embed_test, X_cat_test, X_news_test], axis=1)\n",
    "\n",
    "# 🧬 증강\n",
    "X_aug, y_aug = interpolate_augment(\n",
    "    X_embed_train, X_cat_train, X_news_train, y_train, n_aug=5\n",
    ")\n",
    "\n",
    "\n",
    "X_train_final = np.vstack([X_train_real, X_aug])\n",
    "y_train_final = np.concatenate([y_train, y_aug])\n",
    "print(f\"🧬 증강 완료 ➤ 학습: {X_train_final.shape}, 검증: {X_test.shape}\")\n",
    "\n",
    "# 비교용\n",
    "X_train_augmented = X_train_final  # (원본 + 증강)\n",
    "y_train_augmented = y_train_final\n",
    "\n",
    "# 그래프 + 거리 비교\n",
    "compare_three_distributions(\n",
    "    X_train_real=X_train_real,\n",
    "    X_train_augmented=X_train_augmented,\n",
    "    X_test=X_test,\n",
    "    y_train_real=y_train,\n",
    "    y_train_augmented=y_train_augmented,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "\n",
    "# 🎯 CatBoost 학습\n",
    "print(\"🎯 CatBoostRegressor 학습 중...\")\n",
    "reg = CatBoostRegressor(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    random_seed=42,\n",
    "    od_type='Iter',\n",
    "    od_wait=50,\n",
    "    verbose=100\n",
    ")\n",
    "reg.fit(X_train_final, y_train_final)\n",
    "print(\"✅ 학습 완료!\")\n",
    "\n",
    "# 📈 평가\n",
    "print(\"📊 회귀 성능 평가:\")\n",
    "y_train_pred = reg.predict(X_train_real)\n",
    "y_test_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"🧪 [Train Set]\")\n",
    "print(f\"  🔹 MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  🔹 MAE: {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  🔹 R²:  {r2_score(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "print(\"🧪 [Test Set]\")\n",
    "print(f\"  🔹 MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  🔹 MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  🔹 R²:  {r2_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "131e463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 회귀 성능 평가:\n",
      "🧪 [Train Set]\n",
      "  🔹 MSE: 0.0000\n",
      "  🔹 MAE: 0.0004\n",
      "  🔹 R²:  1.0000\n",
      "🧪 [Test Set]\n",
      "  🔹 MSE: 0.0490\n",
      "  🔹 MAE: 0.1468\n",
      "  🔹 R²:  -0.1581\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# 12. 학습셋 및 테스트셋 성능 평가\n",
    "print(\"\\n📊 회귀 성능 평가:\")\n",
    "\n",
    "# 🔹 학습셋\n",
    "y_train_pred = reg.predict(X_train_real)\n",
    "print(\"🧪 [Train Set]\")\n",
    "print(f\"  🔹 MSE: {mean_squared_error(y_train_real, y_train_pred):.4f}\")\n",
    "print(f\"  🔹 MAE: {mean_absolute_error(y_train_real, y_train_pred):.4f}\")\n",
    "print(f\"  🔹 R²:  {r2_score(y_train_real, y_train_pred):.4f}\")\n",
    "\n",
    "# 🔹 테스트셋\n",
    "y_test_pred = reg.predict(X_test)\n",
    "print(\"🧪 [Test Set]\")\n",
    "print(f\"  🔹 MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  🔹 MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  🔹 R²:  {r2_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5b343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.3)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "     -------------------------------------- 386.6/386.6 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.10.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (3.7.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (0.20.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "     -------------------------------------- 242.5/242.5 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: tomli in c:\\users\\user\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (2.0.1)\n",
      "Collecting typing-extensions>=4.12\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.8/45.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.0.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: typing-extensions, Mako, colorlog, alembic, optuna\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0 typing-extensions-4.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost optuna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
