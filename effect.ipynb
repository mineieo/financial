{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e42a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "ğŸ” ê²°ì¸¡ ì œê±° ë° ì—´ ì¶”ì¶œ ì¤‘...\n",
      "ğŸ§  SBERT ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ğŸ”¡ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ (SBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ ì„ë² ë”© ì§„í–‰:   8%|â–ˆâ–‹                   | 10/123 [02:47<31:30, 16.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# âœ… ë¬¸ì¥ ì„ë² ë”©\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m X_embed \u001b[38;5;241m=\u001b[39m \u001b[43membed_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# âœ… One-hot ì¸ì½”ë”© ì²˜ë¦¬\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”  ë¶„ë¥˜ ë° ê°ì„± One-hot ì¸ì½”ë”© ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36membed_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     32\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ’¬ ì„ë² ë”© ì§„í–‰\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:350\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    347\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 350\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# âœ… ì‹œì‘\n",
    "print(\"ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\íŒŒí”„\\ê¸°ì‚¬_ì „ì²´_í†µí•©ë³¸.xlsx\")\n",
    "df[\"ê¸°ì‚¬ìˆ˜\"] = pd.to_numeric(df[\"ê¸°ì‚¬ìˆ˜\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "X_news_count = df[[\"ê¸°ì‚¬ìˆ˜\"]].values  # shape (n, 1)\n",
    "\n",
    "print(\"ğŸ” ê²°ì¸¡ ì œê±° ë° ì—´ ì¶”ì¶œ ì¤‘...\")\n",
    "category_cols = ['í†µí•© ë¶„ë¥˜1', 'í†µí•© ë¶„ë¥˜2', 'í†µí•© ë¶„ë¥˜3',\n",
    "                 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜1', 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜2', 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜3', 'ê°ì„±']\n",
    "df = df.dropna(subset=[\"ì„¤ëª…ë ¥\", \"íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)\"])\n",
    "y = df[\"ì„¤ëª…ë ¥\"].astype(int)\n",
    "\n",
    "# âœ… SBERT ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ§  SBERT ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "# âœ… ì„ë² ë”© í•¨ìˆ˜\n",
    "def embed_features(df):\n",
    "    print(\"ğŸ”¡ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ (SBERT)...\")\n",
    "    texts = df[\"íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)\"].fillna(\"\").tolist()\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"ğŸ’¬ ì„ë² ë”© ì§„í–‰\"):\n",
    "        emb = model.encode(text, convert_to_numpy=True)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# âœ… ë¬¸ì¥ ì„ë² ë”©\n",
    "X_embed = embed_features(df)\n",
    "\n",
    "# âœ… One-hot ì¸ì½”ë”© ì²˜ë¦¬\n",
    "print(\"ğŸ”  ë¶„ë¥˜ ë° ê°ì„± One-hot ì¸ì½”ë”© ì¤‘...\")\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_cat = onehot.fit_transform(df[category_cols])\n",
    "print(f\"  â¤ ì¸ì½”ë”© ê²°ê³¼ shape: {X_cat.shape}\")\n",
    "\n",
    "# âœ… ì „ì²´ feature í•©ì¹˜ê¸°\n",
    "print(\"ğŸ”— ì„ë² ë”© + ì¸ì½”ë”© í•©ì¹˜ê¸°...\")\n",
    "X_all = np.concatenate([X_embed, X_cat, X_news_count], axis=1)\n",
    "\n",
    "print(f\"  â¤ ìµœì¢… feature shape: {X_all.shape}\")\n",
    "\n",
    "# âœ… í•™ìŠµ-ê²€ì¦ ë¶„í• \n",
    "print(\"ğŸ§ª í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ ë¶„í•  ì¤‘...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42)\n",
    "print(f\"  â¤ í•™ìŠµ ì„¸íŠ¸: {X_train.shape}, ê²€ì¦ ì„¸íŠ¸: {X_test.shape}\")\n",
    "\n",
    "# âœ… ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ğŸ¯ ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\")\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# âœ… ëª¨ë¸ í‰ê°€\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec2c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¡ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ (SBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ ì„ë² ë”© ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [35:27<00:00, 17.29s/it]\n"
     ]
    }
   ],
   "source": [
    "def embed_features(df):\n",
    "    print(\"ğŸ”¡ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ (SBERT)...\")\n",
    "    texts = df[\"íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)\"].fillna(\"\").tolist()\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"ğŸ’¬ ì„ë² ë”© ì§„í–‰\"):\n",
    "        emb = model.encode(text, convert_to_numpy=True)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# âœ… ë¬¸ì¥ ì„ë² ë”©\n",
    "X_embed = embed_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b29a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pickle\n",
    "\n",
    "with open(r\"C:\\Users\\user\\Desktop\\íŒŒí”„\\X_embed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_embed, f)  # ê°€ì¥ ë³´ì¡´ë ¥ ì¢‹ìŒ\n",
    "pd.DataFrame(X_embed).to_csv(r\"C:\\Users\\user\\Desktop\\íŒŒí”„\\X_embed.csv\", index=False)# í˜¸í™˜ì„± ì¢‹ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5ad395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "ğŸ” ê²°ì¸¡ ì œê±° ë° ì—´ ì¶”ì¶œ ì¤‘...\n",
      "ğŸ”  ë¶„ë¥˜ ë° ê°ì„± One-hot ì¸ì½”ë”© ì¤‘...\n",
      "  â¤ ì¸ì½”ë”© ê²°ê³¼ shape: (123, 146)\n",
      "ğŸ”— ì„ë² ë”© + ì¸ì½”ë”© í•©ì¹˜ê¸°...\n",
      "  â¤ ìµœì¢… feature shape: (123, 915)\n",
      "ğŸ§ª í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ ë¶„í•  ì¤‘...\n",
      "  â¤ í•™ìŠµ ì„¸íŠ¸: (98, 915), ê²€ì¦ ì„¸íŠ¸: (25, 915)\n",
      "ğŸ¯ ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\n",
      "âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.92      0.76        12\n",
      "           1       0.88      0.54      0.67        13\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.76      0.73      0.71        25\n",
      "weighted avg       0.77      0.72      0.71        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\íŒŒí”„\\ê¸°ì‚¬_ì „ì²´_í†µí•©ë³¸.xlsx\")\n",
    "df[\"ê¸°ì‚¬ìˆ˜\"] = pd.to_numeric(df[\"ê¸°ì‚¬ìˆ˜\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "X_news_count = df[[\"ê¸°ì‚¬ìˆ˜\"]].values  # shape (n, 1)\n",
    "\n",
    "print(\"ğŸ” ê²°ì¸¡ ì œê±° ë° ì—´ ì¶”ì¶œ ì¤‘...\")\n",
    "category_cols = ['í†µí•© ë¶„ë¥˜1', 'í†µí•© ë¶„ë¥˜2', 'í†µí•© ë¶„ë¥˜3',\n",
    "                 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜1', 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜2', 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜3', 'ê°ì„±']\n",
    "df = df.dropna(subset=[\"ì„¤ëª…ë ¥\", \"íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)\"])\n",
    "y = df[\"ì„¤ëª…ë ¥\"].astype(int)\n",
    "# âœ… One-hot ì¸ì½”ë”© ì²˜ë¦¬\n",
    "print(\"ğŸ”  ë¶„ë¥˜ ë° ê°ì„± One-hot ì¸ì½”ë”© ì¤‘...\")\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_cat = onehot.fit_transform(df[category_cols])\n",
    "print(f\"  â¤ ì¸ì½”ë”© ê²°ê³¼ shape: {X_cat.shape}\")\n",
    "\n",
    "# âœ… ì „ì²´ feature í•©ì¹˜ê¸°\n",
    "print(\"ğŸ”— ì„ë² ë”© + ì¸ì½”ë”© í•©ì¹˜ê¸°...\")\n",
    "X_all = np.concatenate([X_embed, X_cat, X_news_count], axis=1)\n",
    "\n",
    "print(f\"  â¤ ìµœì¢… feature shape: {X_all.shape}\")\n",
    "\n",
    "# âœ… í•™ìŠµ-ê²€ì¦ ë¶„í• \n",
    "print(\"ğŸ§ª í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ ë¶„í•  ì¤‘...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42)\n",
    "print(f\"  â¤ í•™ìŠµ ì„¸íŠ¸: {X_train.shape}, ê²€ì¦ ì„¸íŠ¸: {X_test.shape}\")\n",
    "\n",
    "# âœ… ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ğŸ¯ ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\")\n",
    "clf = RandomForestClassifier(n_estimators=600, random_state=42, max_depth = 10, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# âœ… ëª¨ë¸ í‰ê°€\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1eea0d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:120\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"ì„¤ëª…ë ¥\": desc,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import wasserstein_distance, mode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    ")\n",
    "# ---------------------------------------- #\n",
    "# ğŸ“Œ ì¦ê°• í•¨ìˆ˜\n",
    "def targeted_augment(X_embed, X_cat, X_news, y, ref_news_values, n_aug=3, noise_level=0.01):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for i in range(len(X_embed)):\n",
    "        for _ in range(n_aug):\n",
    "            noise = np.random.normal(0, noise_level, size=X_embed.shape[1])\n",
    "            embed_aug = X_embed[i] + noise\n",
    "            \n",
    "            # ğŸ¯ ê¸°ì‚¬ìˆ˜ ì¦ê°•: í…ŒìŠ¤íŠ¸ ë¶„í¬ì—ì„œ ì§ì ‘ ìƒ˜í”Œë§\n",
    "            news_aug = np.random.choice(ref_news_values)\n",
    "            \n",
    "            x_combined = np.concatenate([embed_aug, X_cat[i], [news_aug]])\n",
    "            Xs.append(x_combined)\n",
    "            ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "                                  \n",
    "def interpolate_augment(X_embed, X_cat, X_news, y, n_aug=3, news_reference=None):\n",
    "    \"\"\"\n",
    "    - ì„ë² ë”©, ì¹´í…Œê³ ë¦¬, íƒ€ê²Ÿì€ ì„ í˜• ë³´ê°„\n",
    "    - ê¸°ì‚¬ìˆ˜(news)ëŠ” ì „ì²´ ë¶„í¬ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§í•˜ì—¬ ë¶„í¬ ì™œê³¡ ë°©ì§€\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    n = len(X_embed)\n",
    "\n",
    "    if news_reference is None:\n",
    "        news_reference = X_news.flatten()\n",
    "\n",
    "    for _ in range(n_aug * n):\n",
    "        # ëœë¤í•˜ê²Œ ë‘ ìƒ˜í”Œ ì„ íƒ\n",
    "        i, j = np.random.randint(0, n, size=2)\n",
    "\n",
    "        alpha = np.random.rand()  # 0~1 ì‚¬ì´ ê°€ì¤‘ì¹˜\n",
    "        embed_mix = (1 - alpha) * X_embed[i] + alpha * X_embed[j]\n",
    "        cat_mix = (1 - alpha) * X_cat[i] + alpha * X_cat[j]\n",
    "        news_mix = np.random.choice(news_reference)  # ì „ì²´ ë¶„í¬ì—ì„œ ê¸°ì‚¬ìˆ˜ ì¶”ì¶œ\n",
    "        y_mix = (1 - alpha) * y[i] + alpha * y[j]\n",
    "\n",
    "        x_combined = np.concatenate([embed_mix, cat_mix, [news_mix]])\n",
    "        Xs.append(x_combined)\n",
    "        ys.append(y_mix)\n",
    "\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------- #\n",
    "# ğŸ“Š ë¶„í¬ ë¹„êµ ì‹œê°í™”\n",
    "def compare_three_distributions(X_train_real, X_train_augmented, X_test, y_train_real, y_train_augmented, y_test):\n",
    "    print(\"\\nğŸ“Š ë¶„í¬ ë¹„êµ (ì›ë³¸ vs ì¦ê°• vs í…ŒìŠ¤íŠ¸)\")\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "\n",
    "    # 1. ê¸°ì‚¬ìˆ˜ ë¶„í¬\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(X_train_real[:, -1], label='Train (ì›ë³¸)', fill=True)\n",
    "    sns.kdeplot(X_train_augmented[:, -1], label='Train (ì¦ê°•)', fill=True)\n",
    "    sns.kdeplot(X_test[:, -1], label='Test', fill=True)\n",
    "    plt.title(\"ğŸ“° ê¸°ì‚¬ìˆ˜ ë¶„í¬\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Y (ìƒê´€ê³„ìˆ˜) ë¶„í¬\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(y_train_real, label='Train Y (ì›ë³¸)', fill=True)\n",
    "    sns.kdeplot(y_train_augmented, label='Train Y (ì¦ê°•)', fill=True)\n",
    "    sns.kdeplot(y_test, label='Test Y', fill=True)\n",
    "    plt.title(\"ğŸ“Š ìƒê´€ê³„ìˆ˜ ë¶„í¬\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ì •ëŸ‰ì  Wasserstein ê±°ë¦¬ë„ ì¶œë ¥\n",
    "    print(f\"ğŸ“ ê¸°ì‚¬ìˆ˜ ê±°ë¦¬ (ì›ë³¸ vs í…ŒìŠ¤íŠ¸):     {wasserstein_distance(X_train_real[:, -1], X_test[:, -1]):.4f}\")\n",
    "    print(f\"ğŸ“ ê¸°ì‚¬ìˆ˜ ê±°ë¦¬ (ì¦ê°• í¬í•¨ vs í…ŒìŠ¤íŠ¸): {wasserstein_distance(X_train_augmented[:, -1], X_test[:, -1]):.4f}\")\n",
    "    print(f\"ğŸ“ Y ê±°ë¦¬ (ì›ë³¸ vs í…ŒìŠ¤íŠ¸):         {wasserstein_distance(y_train_real, y_test):.4f}\")\n",
    "    print(f\"ğŸ“ Y ê±°ë¦¬ (ì¦ê°• í¬í•¨ vs í…ŒìŠ¤íŠ¸):   {wasserstein_distance(y_train_augmented, y_test):.4f}\")\n",
    "\n",
    "# ---------------------------------------- #\n",
    "# ğŸš€ ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\íŒŒí”„\\ê¸°ì‚¬_ì „ì²´_í†µí•©ë³¸ (ìƒê´€ê³„ìˆ˜).xlsx\")\n",
    "df[\"ê¸°ì‚¬ìˆ˜\"] = pd.to_numeric(df[\"ê¸°ì‚¬ìˆ˜\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[df[\"ì„¤ëª…ë ¥\"].isin([0, 1])].copy()\n",
    "df = df.dropna(subset=[\"íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ğŸ“¦ ì„ë² ë”© ë¡œë“œ\n",
    "print(\"ğŸ“¦ ì„ë² ë”© ë¡œë“œ ì¤‘...\")\n",
    "with open(r\"C:\\Users\\user\\Desktop\\íŒŒí”„\\X_embed.pkl\", \"rb\") as f:\n",
    "    all_X_embed = pickle.load(f)\n",
    "X_embed_all = all_X_embed[df.index]\n",
    "assert X_embed_all.shape[0] == len(df)\n",
    "\n",
    "# ğŸ“‚ ê·¸ë£¹ ëŒ€í‘œê°’ êµ¬ì„±\n",
    "category_cols = ['í†µí•© ë¶„ë¥˜1', 'í†µí•© ë¶„ë¥˜2', 'í†µí•© ë¶„ë¥˜3',\n",
    "                 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜1', 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜2', 'ì‚¬ê±´/ì‚¬ê³  ë¶„ë¥˜3', 'ê°ì„±']\n",
    "grouped_rows = []\n",
    "print(\"ğŸ”„ ê·¸ë£¹ë³„ ëŒ€í‘œê°’ ìƒì„± ì¤‘...\")\n",
    "\n",
    "for (desc, corr), group in df.groupby(['ì„¤ëª…ë ¥', 'ìƒê´€ê³„ìˆ˜']):\n",
    "    idxs = group.index.tolist()\n",
    "    news_mean = group[\"ê¸°ì‚¬ìˆ˜\"].mean()\n",
    "    mode_row = group[category_cols].mode().iloc[0]\n",
    "    emb_mean = X_embed_all[idxs].mean(axis=0)\n",
    "\n",
    "    row = {\n",
    "        \"ì„¤ëª…ë ¥\": desc,\n",
    "        \"ìƒê´€ê³„ìˆ˜\": corr,\n",
    "        \"ê¸°ì‚¬ìˆ˜\": news_mean\n",
    "    }\n",
    "    for col in category_cols:\n",
    "        row[col] = mode_row[col]\n",
    "    grouped_rows.append((row, emb_mean))\n",
    "\n",
    "df_new = pd.DataFrame([r for r, _ in grouped_rows])\n",
    "X_embed = np.array([e for _, e in grouped_rows])\n",
    "\n",
    "# ğŸ”¢ ë²”ì£¼í˜• ì¸ì½”ë”©\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_cat = onehot.fit_transform(df_new[category_cols])\n",
    "X_news = df_new[[\"ê¸°ì‚¬ìˆ˜\"]].values\n",
    "y_all = df_new[\"ìƒê´€ê³„ìˆ˜\"].astype(float).values\n",
    "\n",
    "# ğŸ”€ ë°ì´í„° ë¶„í• \n",
    "print(\"âœ‚ï¸ ë°ì´í„° ë¶„ë¦¬ ì¤‘...\")\n",
    "idx_all = np.arange(len(y_all))\n",
    "train_idx, test_idx = train_test_split(idx_all, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split by index\n",
    "X_embed_train, X_embed_test = X_embed[train_idx], X_embed[test_idx]\n",
    "X_cat_train, X_cat_test = X_cat[train_idx], X_cat[test_idx]\n",
    "X_news_train, X_news_test = X_news[train_idx], X_news[test_idx]\n",
    "y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "# í•©ì¹˜ê¸°\n",
    "X_train_real = np.concatenate([X_embed_train, X_cat_train, X_news_train], axis=1)\n",
    "X_test = np.concatenate([X_embed_test, X_cat_test, X_news_test], axis=1)\n",
    "\n",
    "# ğŸ§¬ ì¦ê°•\n",
    "X_aug, y_aug = interpolate_augment(\n",
    "    X_embed_train, X_cat_train, X_news_train, y_train, n_aug=5\n",
    ")\n",
    "\n",
    "\n",
    "X_train_final = np.vstack([X_train_real, X_aug])\n",
    "y_train_final = np.concatenate([y_train, y_aug])\n",
    "print(f\"ğŸ§¬ ì¦ê°• ì™„ë£Œ â¤ í•™ìŠµ: {X_train_final.shape}, ê²€ì¦: {X_test.shape}\")\n",
    "\n",
    "# ë¹„êµìš©\n",
    "X_train_augmented = X_train_final  # (ì›ë³¸ + ì¦ê°•)\n",
    "y_train_augmented = y_train_final\n",
    "\n",
    "# ê·¸ë˜í”„ + ê±°ë¦¬ ë¹„êµ\n",
    "compare_three_distributions(\n",
    "    X_train_real=X_train_real,\n",
    "    X_train_augmented=X_train_augmented,\n",
    "    X_test=X_test,\n",
    "    y_train_real=y_train,\n",
    "    y_train_augmented=y_train_augmented,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "\n",
    "# ğŸ¯ CatBoost í•™ìŠµ\n",
    "print(\"ğŸ¯ CatBoostRegressor í•™ìŠµ ì¤‘...\")\n",
    "reg = CatBoostRegressor(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    random_seed=42,\n",
    "    od_type='Iter',\n",
    "    od_wait=50,\n",
    "    verbose=100\n",
    ")\n",
    "reg.fit(X_train_final, y_train_final)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ğŸ“ˆ í‰ê°€\n",
    "print(\"ğŸ“Š íšŒê·€ ì„±ëŠ¥ í‰ê°€:\")\n",
    "y_train_pred = reg.predict(X_train_real)\n",
    "y_test_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"ğŸ§ª [Train Set]\")\n",
    "print(f\"  ğŸ”¹ MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ MAE: {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ RÂ²:  {r2_score(y_train, y_train_pred):.4f}\")\n",
    "\n",
    "print(\"ğŸ§ª [Test Set]\")\n",
    "print(f\"  ğŸ”¹ MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ RÂ²:  {r2_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "131e463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š íšŒê·€ ì„±ëŠ¥ í‰ê°€:\n",
      "ğŸ§ª [Train Set]\n",
      "  ğŸ”¹ MSE: 0.0000\n",
      "  ğŸ”¹ MAE: 0.0004\n",
      "  ğŸ”¹ RÂ²:  1.0000\n",
      "ğŸ§ª [Test Set]\n",
      "  ğŸ”¹ MSE: 0.0490\n",
      "  ğŸ”¹ MAE: 0.1468\n",
      "  ğŸ”¹ RÂ²:  -0.1581\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# 12. í•™ìŠµì…‹ ë° í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
    "print(\"\\nğŸ“Š íšŒê·€ ì„±ëŠ¥ í‰ê°€:\")\n",
    "\n",
    "# ğŸ”¹ í•™ìŠµì…‹\n",
    "y_train_pred = reg.predict(X_train_real)\n",
    "print(\"ğŸ§ª [Train Set]\")\n",
    "print(f\"  ğŸ”¹ MSE: {mean_squared_error(y_train_real, y_train_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ MAE: {mean_absolute_error(y_train_real, y_train_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ RÂ²:  {r2_score(y_train_real, y_train_pred):.4f}\")\n",
    "\n",
    "# ğŸ”¹ í…ŒìŠ¤íŠ¸ì…‹\n",
    "y_test_pred = reg.predict(X_test)\n",
    "print(\"ğŸ§ª [Test Set]\")\n",
    "print(f\"  ğŸ”¹ MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  ğŸ”¹ RÂ²:  {r2_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5b343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.3)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "     -------------------------------------- 386.6/386.6 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.10.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (3.7.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (0.20.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "     -------------------------------------- 242.5/242.5 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: tomli in c:\\users\\user\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (2.0.1)\n",
      "Collecting typing-extensions>=4.12\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.8/45.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.0.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: typing-extensions, Mako, colorlog, alembic, optuna\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0 typing-extensions-4.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -hap (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost optuna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
